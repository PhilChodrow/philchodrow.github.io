---
title: "The Short Story of Bregman Information for Measuring Segregation" 
author: Phil Chodrow
tags: [information theory, research]
date: 2022-06-24
summary: "A technical look at Bregman information for mathematicians."
echo: false
warning: false
message: false
toc: true
callout-icon: false
callout-appearance: simple
cap-location: margin
bibliography: refs.bib
---

\newcommand{\cP}{\mathcal{P}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\DeclareMathOperator*{\argmax}{argmax\;}
\DeclareMathOperator*{\argmin}{argmin\;}


This is a post for folks who are interested in some of the most important mathematical properties of Bregman divergences. Bregman divergences offer a flexible platform from which to derive measures of diversity and segregation, and folks who wish to engineer such measures may benefit from understanding some useful properties. 

# Audience

The primary audience for these notes consists of folks with training in math and/or machine learning theory. I'll assume familiarity with: 

- The gradient operator $\nabla$. 
- Joint, marginal, and conditional probability distributions on discrete sets. 
- [Convex functions](https://en.wikipedia.org/wiki/Convex_function). 
- Prior background in [Shannon information theory](https://en.wikipedia.org/wiki/Information_theory#:~:text=Information%20theory%20is%20the%20scientific,Claude%20Shannon%20in%20the%201940s.) is helpful but not assumed. I tell this story in my preferred sequence in @chodrow2017divergence. 


# Introducing Bregman Divergences

::: {.callout-note}

## Bregman Divergence

Let $\cP \subseteq \R^n$ be a [convex set](https://en.wikipedia.org/wiki/Convex_set). Let $f:\cP \rightarrow \R$ be a continuously differentiable and [convex function](https://en.wikipedia.org/wiki/Convex_function). Then, the *Bregman divergence* induced by $f$ on $\cP$ is the function $d_f:\cP^2 \rightarrow \R$ given by 

$$
d_f(p,q) = f(p) - f(q) - \langle \nabla f(q), p-q \rangle\;.
$$
:::

[Some more detail on [Wikipedia](https://en.wikipedia.org/wiki/Bregman_divergence).]{.aside}
You can think of $d_f$ as a function that compares the two elements $p$ and $q$ of $\cP$. Bregman divergences are not distances, but they do have some similar properties. Here are some of them: 

::: {.callout-tip}
::: {#thm-bregman-basics}

## Fundamental Properties of Bregman Divergences

For any continuously differentiable and convex $f$: 

- $d_f(p, p) = 0$ for all $p \in \cP$. 
- $d_f(p,q) \geq 0$  for all $p, q \in \cP$. 
- If $f$ is strictly convex, then $d_f(p,q) = 0$ iff $p = q$. 

On the other hand, $d_f(p, q) \neq d_f(q,p)$ in general, and there is no triangle inequality. 

:::
:::

@thm-bregman-basics tells us that we can think of $d_f$ as a comparison operator, but it is important to remember that it is **not** in general a metric on $\cP$. Proofs for @thm-bregman-basics are outlined on [Wikipedia](https://en.wikipedia.org/wiki/Bregman_divergence). 

::: {.callout-note}

## Examples of Bregman Divergences

1. Let $\cX$ be a finite set with $\abs{\cX} = n$. Let $\cP_\cX$ be the set of all discrete probability distributions over the elements of $\cX$. Let $f(p) = \sum_{x \in \cX} p(x) \log p(x)$. Then, $d_f$ is the *Kullback-Leibler divergence*, given by 
$$
d_f(p,q) = \sum_{x \in \cX} p(x) \log \frac{p(x)}{q(x)}\;. 
$$
This follows a direct calculation of the gradient $\nabla f$ and subsequent simplification.  
2. Let $\cP = \R^n$ and let $f(x) = \norm{x}^2$, the squared Euclidean norm. Then, $d_f$ is the squared Euclidean distance: 
$$
d_f(x, y) = \norm{x - y}^2\;.
$$

:::


# Bregman Information

## Motivation From Segregation Measurement

What does it mean to say that a city, for example, is segregated by class? One natural image is of a city separated by a line, with high-wealth individuals on one side and low-wealth individuals on another. One framing of this situation is in terms of information and correlation: 

- Knowing *where someone lives* makes you more able to guess *their wealth*. ("They live on the east side, have you seen those houses?")
- Knowing *someone's wealth* makes you more able to guess *where they live*. ("Folks with that salary tend to settle on the east side.")

The concept of Bregman information can be used to make precise what it means for one attribute (like a residential location) to give information about another attribute (like wealth). 

## Information As Approximation

Let $\cX$ and $\cY$ be two finite sets. Let $p_{XY}$ be a probability distribution on $\cX\times \cY$. That is, if I pick a random object from $\cX\times \cY$, the probability that this object has property $x \in \cX$ and $y \in \cY$ is $p_{XY}(x,y)$. Concretely, think of $X$ as being a demographic feature like wealth and $Y$ as being a spatial location. Then, $p_{XY}(\text{high wealth}, \mathrm{Providence})$ is the fraction of all people who have high wealth and live in Providence. We'll develop an information measure to  quantify the extent to which $X$ and $Y$ are dependent or correlated. I'll treat the extent of dependence or correlation as an operating definition of "demographic spatial segregation." 

We develop the idea of $X$ and $Y$ as being dependent by considering the conditional distributions $p_{X|y}(x) = \frac{p_{XY}(x,y)}{p_Y(y)}$. 
[The notation $p_{X|y}(x)$ is nonstandard, and is chosen to make some later calculations a bit more compact.]{.aside} Here, $p_Y(y) = \sum_{x\in \cX} p_{XY}(x,y)$ is the marginal distribution of $Y$. You can think of $p_{X|y}(x)$ as the probability that an individual in location $y$ has demographic attribute $x$. The distribution $p_{X|y}$ is an element of $\cP_{\cX}$, the set of probability distributions over $\cX$. We have one of those distributions for each possible choice of $y \in \cY$. If these distributions are very different -- if they depend strongly on the choice of $y$ -- then knowing $y$ would make a big difference in what we know about $x$. We would say that $X$ and $Y$ are highly dependent. How do we measure how different the collection of distributions $\{p_{X|y}\}$ is? A standard, two-step approach is: 


1. First, find a point $\bar{p}_X$ that best approximates all the marginal distributions $p_{X|y}$. 
2. Compute the average divergence of the conditionals to $\bar{p}_X$. This average is 
$$
I_f(X:Y;\bar{p}_X) = \sum_{y \in \cY} p_Y(y) d_f(p_{X|y}, \bar{p}_X)
$$
This is a gnarly expression, but you can think of as expressing: on average, how far (in the sense of divergence $d_f$) is a typical local distribution $p_{X|y}$ from its approximator $\bar{p}_X$? If the answer is small, then we know that all the local distributions aren't *too far* from their approximators, and therefore shouldn't be *too far* from each other, either. 

As it turns out, there's a correct choice of $\bar{p}_X$. This result is Proposition 1 of @banerjee2005clustering. 

::: {.callout-tip}
::: {#thm-mean-minimizer}

## Mean is a Minimizer

For any Bregman divergence, the smallest value of $I_f(X:Y;\bar{p}_X)$ is attained when $\bar{p}_X = p_X$, the marginal distribution of $X$. This distribution is given by $p_X(x) = \sum_{y \in Y}p_{XY}(x,y)$. More formally, 

$$
\argmin_{\bar{p}_X} I_f(X:Y;\bar{p}_X) = p_X\;.
$$

:::
:::

::: {.callout-note}

## Bregman Information

We will use the abbreviation $I_f(X: Y) = I_f(X:Y;p_X)$ for the minimizing value of $I_f(X:Y;\bar{p}_X)$. We call $I_f(X:Y)$ the **Bregman information** in $Y$ about $X$. 

:::

::: {.callout-tip}
::: {#thm-independence}

## Bregman Information and Independence

Let $f$ be strictly convex and let $p_Y(y) > 0$ for all $y \in \cY$. Then, $I_f(X:Y) = 0$ if and only if $X$ and $Y$ are independent random variables. 
:::
:::

::: {.proof}
Recall that $I_f(X:Y) = \sum_{y \in \cY} p_Y(y) d_f(p_{X|y}, p_X)$. Since $p_Y(y) > 0$ for each $y \in \cY$ by hypothesis, for $I_f(X:Y) = 0$ we must have $d_f(p_{X|y}, p_X)$ for each $y$. If $f$ is strictly convex, then by @thm-bregman-basics, $d_f(p,q) = 0$ iff $p = q$. So, we must have $p_{X|y} = p_X$ for each $y$, which implies that $X$ and $Y$ are independent. 
:::



# The Chain Rule


From a practical perspective, one of the most important properties of the Bregman information is the chain rule. We'll now state and prove it. 
[This result is a rewording of Theorem 1 in @banerjee2005clustering.]{.aside}

Let $\cZ$ be a finite set, and let $g: \cY \rightarrow \cZ$. Let $Z = g(Y)$. We write $p_{X|z}(x) = \frac{\sum_{Y \in g^{-1}(z)} p_{XY}(x,y)}{\sum_{Y \in g^{-1}(z)} p_Y(y)}$ for the distribution of $X$ conditioned on the event $Z = z$. We similarly write $p_{Y|z}$ for the distribution of $y$ conditioned on $Z = z$. 

::: {.callout-note}

## Conditional Bregman Information

The **conditional Bregman information in $Y$ about $X$ given $Z$**, written $I(X:Y|Z)$, is 
$$
I(X:Y|Z) = \sum_{z \in Z} p_Z(z) I_f(X:Y|z) = \sum_{z \in \cZ} p_Z(z) \sum_{y \in \cY}p_{Y|z}(y) d_f(p_{X|y}, p_{X|z})\;. 
$$
:::
You can think of $I_f(X:Y|z)$ as the information that $Y$ contains about $X$ in a world in which $g(Y)$ is always equal to $z$. The conditional information $I(X:Y|Z)$ is the average across values of $z$. 

::: {.callout-tip}
::: {#thm-chain-rule}

## Chain Rule of Bregman Information

If $Z = g(Y)$ for some function $g$, then 

$$
I_f(X:Y) = I_f(X:Z) + I_f(X:Y|Z)
$$

:::
:::

::: {.proof}
We'll start by writing 

$$
\begin{aligned}
I_f(X:Y) &= \sum_{y \in \cY} p_Y(y) d_f(p_{X|y}, p_X) \\ 
         &= \sum_{z \in \cZ} \sum_{y \in g^{-1}(z)} p_Y(y) d_f(p_{X|y}, p_X) \\ 
         &= \sum_{z \in \cZ} \sum_{y \in g^{-1}(z)} p_Y(y) \left[d_f(p_{X|y}, p_{X|z}) + d_f(p_{X|z}, p_X) + \langle  \nabla f(p_{X|z}) - \nabla f(p_X),  p_{X|y} - p_{X|z}\rangle \right] 
\end{aligned}
$$

This last line is the "law of cosines"[See [Wikipedia](https://en.wikipedia.org/wiki/Bregman_divergence#Properties)]{.aside} for Bregman divergences and follows by just rearranging some terms. Let's break this final line down into three terms. 

First, 

$$
\begin{aligned}
\sum_{z \in \cZ} \sum_{y \in g^{-1}(z)} p_Y(y) d_f(p_{X|y}, p_{X|z}) 
&= \sum_{z \in \cZ} p_{Z}(z) \sum_{y \in g^{-1}(z)} \frac{p_Y(y)}{p_Z(z)} d_f(p_{X|y}, p_{X|z}) \\ 
&= \sum_{z \in \cZ} p_{Z}(z) \sum_{y \in g^{-1}(z)} p_{Y|z}(y) d_f(p_{X|y}, p_{X|z}) \\ 
&= I_f(X:Y|Z)\;.
\end{aligned}
$$

Second, 

$$
\begin{aligned}
\sum_{z \in \cZ} \sum_{y \in g^{-1}(z)} p_Y(y)d_f(p_{X|z}, p_X) = \sum_{z \in \cZ} p_Z(z)d_f(p_{X|z}, p_X) = I_f(X:Z)\;.
\end{aligned}
$$

Third and finally, the very last term is equal to 0 because 

$$
\begin{aligned}
\sum_{y \in g^{-1}(z)}p_Y(y)p_{X|y}(x) = p_{X|z}(x)\;.
\end{aligned}
$$
for all $x$. 

So, we have shown that $I_f(X:Y) = I_f(X:Z) + I_f(X:Y|Z)$, completing the proof. 
:::

You can think of the chain rule as saying the following: 

> The information in $Y$ about $X$ is equal to the information that $Z$ holds about $X$, plus the *additional* information that $Y$ holds in a world in which you already know $Z$. 

# Bregman Information Geometry

A Bregman divergence is not a metric. @thm-bregman-basics highlights some ways in which a Bregman divergence is like a metric. There is, however, another important way. In brief, Bregman divergences between "nearby" vectors behave like *[Riemannian metrics](https://en.wikipedia.org/wiki/Riemannian_geometry)*, and therefore allow us to do geometry. Here's a concrete version of this idea: 

::: {.callout-tip}
::: {#thm-riemannian}
The Hessian (matrix of second derivatives) $H(q)$ of $f$ at point $q$ is positive semi-definite for any $q$, and positive-definite if $f$ is strictly convex.  Furthermore, there exists $\epsilon > 0$ such that, if $\norm{p - q} < \epsilon$, then 
$$
d_f(p, q) = \frac{1}{2} \langle p-q,H(q)(p-q) \rangle + O(\epsilon^2)
$$
:::
:::

::: {.proof}
Positive (semi)-definiteness of $H(q)$ follows from standard results about the Hessians of convex functions. [See e.g. [these notes](https://cims.nyu.edu/~cfgranda/pages/OBDA_fall17/notes/convex_optimization.pdf), Cor. 2.18 and Remark 2.19]{.aside} To prove the equation, we Taylor-expand $d_f(p, q)$ in the first argument around the point $q$, which we can do in an $\epsilon$-neighborhood for $\epsilon$ chosen sufficiently small. 

$$
d_f(p, q) = d_f(q, q) + \langle \nabla_1 d_f(q, q),p - q \rangle + \frac{1}{2} \langle p-q, \bar{H}(p-q)\rangle + O(\norm{p - q}^2)\;.
$$

Here, $\nabla_1 d_f(q, q)$ indicates that derivatives are taken only in the first argument of $d_f$. $\bar{H}$ is the Hessian matrix of $d_f$, again with derivatives taken only in the first argument. 

Let's evaluate terms. We have $d_f(p, p) = 0$ by @thm-bregman-basics. Moving to the second term, we have 
$$
\begin{aligned}
\langle \nabla_1 d_f(q, q), p-q \rangle = \langle \nabla f(q) -  \nabla f(q), p-q \rangle \rangle  = 0
\end{aligned}
$$

It remains to calculate $\bar{H}$. When taking two derivatives with respect to the first argument of $d_f$, only the term initially corresponding to $f(p)$ survives, and we therefore have $\bar{H} = H$, the Hessian of $H$. Finally, noting that $\norm{p - q} < \epsilon$ by construction completes the argument. 
:::

One reason that @thm-riemannian is so good is that it allows us actually get "real" metrics from divergences. From the simplest perspective, we know that, for $p$ and $q$ close, $d_f(p,q) \approx \frac{1}{2} \langle p-q,H(q)(p-q)\rangle$, which is a metric when $f$ is strictly convex (as $H$ is then positive-definite). On the other hand, we can also use the perspective of Riemannian geometry, under which $H$ plays the role of an *ambient metric tensor*. Many problems in statistical inference can be framed in terms exploring the structure of a *manifold* of probability distributions. Any smooth parameterized family of distributions defines such a manifold; for example, the Poisson distribution $p_K(k;\lambda) = \frac{e^{\lambda}\lambda^k}{k!}$ is parameterized by a single parameter $\lambda$. We can compare these distributions using the ambient metric, and the result is a *Riemannian metric* induced on the 1-dimensional manifold of distributions. 

For more on information geometry, see @nielsen2020elementary or @amari2016information. 

# Information Geometry and Spatial Analysis

The information geometry point can feel pretty abstract, but there's a specific reason to be interested in this perspective when studying spatial segregation. In particular, consider the following idealized model of Census data. Let $S$ be a connected, closed region in $\R^2$. For each point $s \in S$, we have a probability distribution $p_{X|s}$ over some set of demographic variables $\cX$. 

We now impose the assumption that $p_{X|s}$ is smooth as a function of $s$. This allows us to apply @thm-riemannian, which in turn allows us to treat $S$ as a Riemannian manifold and do geometric calculations in a metric that is adapted to the demographic structure. This allow us to do things like measure "demographic distance" between points in space, and also enables some novel approaches to characterizing the spatial scale of segregation [@chodrow2017structure]. 
